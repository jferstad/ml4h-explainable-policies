{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a06994c-7295-4a9a-8440-05b6f30029a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# pip install econml shap \"flaml[automl]\" dill plotnine scikit-misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa3ec77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8802dadf-9756-4337-81bc-0153bf4ad397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import dill as pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from econml.dml import CausalForestDML, SparseLinearDML\n",
    "from econml.dr import SparseLinearDRLearner, ForestDRLearner\n",
    "from econml.metalearners import XLearner, TLearner, SLearner\n",
    "\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.base import clone, BaseEstimator, clone\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, KFold, StratifiedKFold, cross_val_predict, cross_val_score, train_test_split, GroupShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import flaml\n",
    "from flaml import AutoML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotnine as p9\n",
    "import shap\n",
    "import math\n",
    "\n",
    "import scipy\n",
    "import scipy.special\n",
    "from statsmodels.api import OLS\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7bd73-64bc-4156-ad51-6b94c7f185c4",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c9c6224-0bf9-4b89-a63c-370a2ab14488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../output/pretraining/representations_combo_df_w_lags.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edd9c82-28e9-4377-a92c-1b9b84a058b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIDE_VARS = [\n",
    "    'very_low_7dr', 'low_7dr', 'in_range_7dr', 'g_7dr', 'using_pump',\n",
    "    'in_range_7dr_7d_delta', 'large_tir_drop',\n",
    "    'low_tir', 'lows', 'very_lows', \n",
    "    'pop_4T_1', 'pop_4T_2', 'pop_TIPS', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dadc5585-f914-4d45-8f23-c609cbff03ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196816, 122)\n",
      "(129345, 122)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with no flags\n",
    "df['large_tir_drop'] = ((df['in_range_7dr_7d_delta'] < -0.15) & (df['time_worn_7dr'] > 0.5)).astype(int)\n",
    "df['low_tir'] = ((df['in_range_7dr'] < 0.65) & (df['time_worn_7dr'] > 0.5)).astype(int)\n",
    "df['lows'] = (df['low_7dr'] > 0.04).astype(int)\n",
    "df['very_lows'] = (df['very_low_7dr'] > 0.01).astype(int)\n",
    "\n",
    "print(df.shape)\n",
    "df = df[(df.low_tir+df.lows+df.very_lows+df.large_tir_drop)>0]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce019f6-ff2c-4d04-8eb9-2ff1dd5ca752",
   "metadata": {},
   "source": [
    "# AutoML to fit nuisance models on TIDE variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68a2f3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to only use W variables when fitting and precting with nuisance models.\n",
    "# In EconML, the X [features for CATE] and W [features for nuisances] are h-stacked. \n",
    "# We want to only us the W features (i.e. last W.shape[0] columns of XW)\n",
    "# https://github.com/py-why/EconML/blob/main/econml/dml/dml.py#L44\n",
    "\n",
    "\n",
    "###################################\n",
    "# AutoML models\n",
    "###################################\n",
    "\n",
    "# FLAML models don't return \"self\" at end of fit. We create this wrapper.\n",
    "\n",
    "class AutoMLWrap(BaseEstimator):\n",
    "\n",
    "    def __init__(self, *, model, automl, features, filter_feats):\n",
    "        self.model = model\n",
    "        self.automl = automl\n",
    "        self.features = features\n",
    "        self.filter_feats = filter_feats\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model_ = clone(self.model)\n",
    "        Xf = X[:,-len(self.features):] if self.filter_feats else X\n",
    "        self.model_.fit(Xf, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xf = X[:,-len(self.features):] if self.filter_feats else X\n",
    "        return self.model_.predict(Xf)\n",
    "\n",
    "# Custom r2 loss for regression, for more trustworthy learning curves.\n",
    "def reg_r2(\n",
    "        X_val, y_val, estimator, labels,\n",
    "        X_train, y_train, weight_val=None, weight_train=None,\n",
    "        *args,):\n",
    "    mse = np.mean((estimator.predict(X_val) - y_val)**2)\n",
    "    r_2 = 1-mse/np.mean((y_val - y_val.mean())**2)\n",
    "    return -1*r_2, {\"val_loss\": r_2}\n",
    "\n",
    "def auto_reg(X, y, *, features,\n",
    "             groups=None, n_splits=5, split_type='auto', time_budget=60, verbose=0, \n",
    "             estimator_list='auto', log_file_name='flaml_log.txt'):\n",
    "    X = np.array(X)\n",
    "    automl = AutoML(task='regression', time_budget=time_budget, early_stop=True,\n",
    "                    eval_method='cv', n_splits=n_splits, split_type=split_type,\n",
    "                    metric=reg_r2, verbose=verbose, estimator_list=estimator_list)\n",
    "    if groups is None:\n",
    "        automl.fit(X, y, log_file_name=log_file_name)\n",
    "    else:\n",
    "        automl.fit(X, y, groups=groups, log_file_name=log_file_name)\n",
    "    best_est = automl.best_estimator\n",
    "    return lambda x=False: AutoMLWrap(model=clone(automl.best_model_for_estimator(best_est)), automl=automl, features=features, filter_feats=x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f7e503c-3689-4d18-ae8c-d7b3871d2167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoMLWrapCLF(BaseEstimator):\n",
    "\n",
    "    def __init__(self, *, model, automl, prop_lb, features, filter_feats):\n",
    "        self.model = model\n",
    "        self.automl = automl\n",
    "        self.prop_lb = prop_lb\n",
    "        self.features = features\n",
    "        self.filter_feats = filter_feats\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model_ = clone(self.model)\n",
    "        Xf = X[:,-len(self.features):] if self.filter_feats else X\n",
    "        self.model_.fit(Xf, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xf = X[:,-len(self.features):] if self.filter_feats else X\n",
    "        preds = self.model_.predict_proba(Xf) \n",
    "        preds = np.clip(preds, self.prop_lb, 1-self.prop_lb)\n",
    "        return preds\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        Xf = X[:,-len(self.features):] if self.filter_feats else X\n",
    "        preds = self.model_.predict_proba(Xf) \n",
    "        preds = np.clip(preds, self.prop_lb, 1-self.prop_lb)\n",
    "        return preds\n",
    "\n",
    "# Custom r2 loss for classification, for more trustworthy learning curves.\n",
    "def clf_r2(\n",
    "        X_val, y_val, estimator, labels,\n",
    "        X_train, y_train, weight_val=None, weight_train=None,\n",
    "        *args,):\n",
    "    mse = np.mean((estimator.predict_proba(X_val)[:, 1] - y_val)**2)\n",
    "    r_2 = 1-mse/np.mean((y_val - y_val.mean())**2)\n",
    "    return -1*r_2, {\"val_loss\": r_2}\n",
    "\n",
    "def clf_mod_log_loss(\n",
    "    X_val, y_val, estimator, labels,\n",
    "    X_train, y_train, weight_val=None, weight_train=None,\n",
    "    *args,):\n",
    "    \n",
    "    preds = estimator.predict_proba(X_val)[:,1]\n",
    "\n",
    "    mod_log_loss = np.mean(-1* ( (.01 + y_val)*np.log(preds) + (1.01 - y_val)*np.log(1-preds)))\n",
    "\n",
    "    return mod_log_loss, {\"val_loss\": mod_log_loss}\n",
    "\n",
    "def auto_clf(\n",
    "        X, y, *, features,\n",
    "        groups=None, n_splits=5, split_type='auto', time_budget=60, verbose=0, estimator_list='auto', \n",
    "        log_file_name='flaml_log.txt', prop_lb=0.02):\n",
    "    X = np.array(X)\n",
    "    automl = AutoML(task='classification', time_budget=time_budget, early_stop=True,\n",
    "                    eval_method='cv', n_splits=n_splits, split_type=split_type,\n",
    "                    metric='log_loss', verbose=verbose, estimator_list=estimator_list,\n",
    "                   )\n",
    "    if groups is None:\n",
    "        automl.fit(X, y, log_file_name=log_file_name)\n",
    "    else:\n",
    "        automl.fit(X, y, groups=groups, log_file_name=log_file_name)\n",
    "    best_est = automl.best_estimator\n",
    "    return lambda x=False: AutoMLWrapCLF(model=clone(automl.best_model_for_estimator(best_est)), automl=automl, prop_lb=prop_lb, features=features, filter_feats=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f307888-b2f1-4497-8913-b9bec92d49ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "df_train = df[df.data_split=='train'].copy()\n",
    "df_val = df[df.data_split=='val'].copy()\n",
    "df_test = df[df.data_split=='test'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2838316-56d6-43bb-8111-89a0b6f56481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTROL_VARS = [\n",
    "    'very_low_7dr', 'low_7dr', 'in_range_7dr', 'g_7dr', 'using_pump',\n",
    "    'in_range_7dr_7d_delta', 'large_tir_drop',\n",
    "    'low_tir', 'lows', 'very_lows', \n",
    "    'pop_4T_1', 'pop_4T_2', 'pop_TIPS'\n",
    "]\n",
    "\n",
    "LAGS_1W = ['lag_1_very_low_7dr','lag_1_low_7dr','lag_1_in_range_7dr','lag_1_g_7dr','lag_1_in_range_7dr_7d_delta','lag_1_large_tir_drop','lag_1_low_tir','lag_1_lows','lag_1_very_lows','lag_1_received_message']\n",
    "LAGS_2W = ['lag_2_very_low_7dr','lag_2_low_7dr','lag_2_in_range_7dr','lag_2_g_7dr','lag_2_in_range_7dr_7d_delta','lag_2_large_tir_drop','lag_2_low_tir','lag_2_lows','lag_2_very_lows','lag_2_received_message']       \n",
    "LAGS_3W = ['lag_3_very_low_7dr','lag_3_low_7dr','lag_3_in_range_7dr','lag_3_g_7dr','lag_3_in_range_7dr_7d_delta','lag_3_large_tir_drop','lag_3_low_tir','lag_3_lows','lag_3_very_lows','lag_3_received_message']       \n",
    "LAGS_4W = ['lag_4_very_low_7dr','lag_4_low_7dr','lag_4_in_range_7dr','lag_4_g_7dr','lag_4_in_range_7dr_7d_delta','lag_4_large_tir_drop','lag_4_low_tir','lag_4_lows','lag_4_very_lows','lag_4_received_message']       \n",
    "\n",
    "CONTROL_VARS = CONTROL_VARS + LAGS_1W + LAGS_2W + LAGS_3W #+ LAGS_4W\n",
    "CONTROL_VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca5a2a-78a2-4d5e-b192-c189b75ee908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill NAs\n",
    "fill_map = {x: 0 for x in CONTROL_VARS}\n",
    "df_train = df_train.fillna(fill_map)\n",
    "df_val = df_val.fillna(fill_map)\n",
    "df_test = df_test.fillna(fill_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a174dad-d723-4eb9-bec7-9413c365a349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit nuisance models\n",
    "\n",
    "TESTING = False\n",
    "PROP_LB = 0.01\n",
    "\n",
    "time_budget = 1 if TESTING else 300 \n",
    "verbose = 1  # verbosity of auto-ml\n",
    "n_splits = 10 # cross-fitting and cross-validation splits\n",
    "\n",
    "X = df_train[CONTROL_VARS].astype(float).to_numpy()\n",
    "Y = df_train['delta_in_range_fw_7d'].values\n",
    "D = df_train['recommends_insulin_dose_change'].values\n",
    "groups = df_train.mrn.values\n",
    "\n",
    "# AutoML Models\n",
    "model_y = auto_reg(\n",
    "    X, Y, features=CONTROL_VARS, groups=groups, n_splits=n_splits, split_type='auto',\n",
    "    verbose=verbose, time_budget=time_budget, estimator_list=['rf'])\n",
    "model_t = auto_clf(\n",
    "    X, D, features=CONTROL_VARS, groups=groups, n_splits=n_splits, split_type='auto',\n",
    "    verbose=verbose, time_budget=time_budget, estimator_list=['rf'], prop_lb=PROP_LB)\n",
    "model_reg_one = auto_reg(\n",
    "    X[D==1], Y[D==1], features=CONTROL_VARS, groups=groups[D==1], n_splits=n_splits, split_type='auto',\n",
    "    verbose=verbose, time_budget=time_budget, estimator_list=['rf'])\n",
    "\n",
    "# # Load pickled stuff\n",
    "# with open(f'../../output/analysis/reps_grid_search_5/model_y.pkl', 'rb') as f:\n",
    "#     model_y = pickle.load(f)\n",
    "    \n",
    "# with open(f'../../output/analysis/reps_grid_search_5/model_t.pkl', 'rb') as f:\n",
    "#     model_t = pickle.load(f)\n",
    "    \n",
    "# with open(f'../../output/analysis/reps_grid_search_5/model_reg_one.pkl', 'rb') as f:\n",
    "#     model_reg_one = pickle.load(f)\n",
    "\n",
    "# # Test\n",
    "# print(model_y(True).fit(X, Y).predict(X))\n",
    "# # print(model_reg_zero().fit(X, Y).predict(X))\n",
    "# print(model_reg_one(True).fit(X, Y).predict(X))\n",
    "# print(model_t(True).fit(X, D).predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9b277-5d61-47a9-9be5-c9e889fcec2a",
   "metadata": {},
   "source": [
    "# Shared Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb09c138-4607-4f63-8324-7d233c2aede6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate DR scores\n",
    "def calculate_dr_outcomes(Xtrain, Dtrain, ytrain, groupstrain, Xval, Dval, yval, groupsval, model_t, model_reg_zero, model_reg_one, clipping):\n",
    "    reg_zero = model_reg_zero().fit(Xtrain[list(Dtrain==0)], ytrain[list(Dtrain==0)])\n",
    "    reg_one = model_reg_one().fit(Xtrain[list(Dtrain==1)], ytrain[list(Dtrain==1)])\n",
    "    reg_zero_preds_t = reg_zero.predict(Xval)\n",
    "    reg_one_preds_t = reg_one.predict(Xval)\n",
    "    reg_preds_t = reg_zero_preds_t * (1 - Dval) + reg_one_preds_t * Dval\n",
    "    prop_preds = model_t().fit(Xtrain, Dtrain).predict(Xval)[:,1]\n",
    "    dr = reg_one_preds_t - reg_zero_preds_t\n",
    "    reisz = (Dval - prop_preds) / np.clip(prop_preds * (1 - prop_preds), clipping, np.inf)\n",
    "    dr += (yval - reg_preds_t) * reisz\n",
    "    return dr\n",
    "\n",
    "\n",
    "# Qini statistic\n",
    "def calc_qini(cate_est, dr_scores, colsuffix):\n",
    "    \n",
    "    # Grid of values of % treated 5-100%\n",
    "    ugrid = np.linspace(0, 95, 39)\n",
    "\n",
    "    toc, toc_std, group_prob = np.zeros(len(qs)), np.zeros(len(qs)), np.zeros(len(qs))\n",
    "    true_toc = np.zeros(len(qs))\n",
    "    toc_psi = np.zeros((len(qs), dr_scores.shape[0]))\n",
    "    n = len(dr_scores)\n",
    "    ate = np.mean(dr_scores)\n",
    "    for it in range(len(qs)):\n",
    "        inds = (qs[it] <= cate_est) # group with larger CATE prediction than the q-th quantile\n",
    "        group_prob = np.sum(inds) / n # fraction of population in this group\n",
    "        toc[it] = group_prob * (np.mean(dr_scores[inds]) - ate) # tau(q) = q * E[Y(1) - Y(0) | tau(X) >= q[it]] - E[Y(1) - Y(0)]\n",
    "        toc_psi[it, :] = (dr_scores - ate) * (inds - group_prob) - toc[it] # influence function for the tau(q)\n",
    "        toc_std[it] = np.sqrt(np.mean(toc_psi[it]**2) / n) # standard error of tau(q)\n",
    "    \n",
    "    # Qini statistic\n",
    "    qini_psi = np.sum(toc_psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "    qini = np.sum(toc[:-1] * np.diff(ugrid) / 100)\n",
    "    qini_stderr = np.sqrt(np.mean(qini_psi**2) / n)\n",
    "    return(pd.DataFrame({\n",
    "        f'QINI_{colsuffix}': [qini],\n",
    "        f'QINI_se_{colsuffix}': [qini_stderr]\n",
    "    }))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b899477-6a27-4d4a-aefa-7bc7f2524a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate CATE function\n",
    "def est_cate_fn(estimator_name, prop_lb, model_y, model_t, Y, X, W, D, n_splits, groups):\n",
    "    \n",
    "    if estimator_name == 'CF':\n",
    "        est = CausalForestDML(\n",
    "            discrete_treatment=True,\n",
    "            model_t=model_t(True),\n",
    "            model_y=model_y(True),\n",
    "            use_ray=False,\n",
    "            cv=GroupKFold(n_splits=n_splits),\n",
    "            random_state=123,\n",
    "            # Same default params as grf Causal Forest\n",
    "            n_estimators=4 if TESTING else 2000,\n",
    "            min_balancedness_tol=0.45, # 0.05\n",
    "            min_samples_split=1e-6,\n",
    "            min_samples_leaf=20,\n",
    "            max_features=min(math.ceil(math.sqrt(X.shape[1])) + 20, X.shape[1]),\n",
    "            max_samples=0.5,\n",
    "            subforest_size=2,\n",
    "        )\n",
    "        est.fit(Y, D, X=X, W=W, groups=groups)\n",
    "    \n",
    "    elif estimator_name == 'DRForest':\n",
    "        est = ForestDRLearner(\n",
    "            model_propensity=model_t(True),\n",
    "            model_regression=model_y(True),\n",
    "            random_state=123,\n",
    "            cv=GroupKFold(n_splits=n_splits),\n",
    "            min_propensity=prop_lb,\n",
    "            n_estimators=4 if TESTING else 2000,\n",
    "        )\n",
    "        est.fit(Y, D, X=X, W=W, groups=groups)\n",
    "        \n",
    "    elif estimator_name == 'XLearner':\n",
    "        est = XLearner(\n",
    "            models = model_y(),\n",
    "            cate_models = model_y(),\n",
    "            propensity_model = model_t(True)\n",
    "        )\n",
    "        est.fit(Y, D, X=X)\n",
    "    \n",
    "    elif estimator_name == 'TLearner':\n",
    "        est = TLearner(\n",
    "            models = model_y(),\n",
    "        )\n",
    "        est.fit(Y, D, X=X)\n",
    "        \n",
    "    elif estimator_name == 'SLearner':\n",
    "        est = SLearner(\n",
    "            overall_model = model_y(),\n",
    "        )\n",
    "        est.fit(Y, D, X=X)\n",
    "    \n",
    "    else:\n",
    "        raise NameError('Incorrect estimator_name in est_cate_fn')\n",
    "    \n",
    "    return(est)  \n",
    "\n",
    "\n",
    "# Test All\n",
    "# W = X\n",
    "# Dm = df_train.custom_treatment.to_numpy()\n",
    "# for x in [\"CF\",\"DRForest\",\"XLearner\",\"TLearner\",\"SLearner\"]:\n",
    "#     print(x)\n",
    "#     est_cate_fn(x, 0.01, model_y, model_t, Y, X, W, Dm, n_splits, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "924b099d-5128-4d27-b235-a20c15abdf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate multinomial DR scores\n",
    "def calculate_dr_outcomes_mn(Xtrain, Dtrain, ytrain, groupstrain, Xval, Dval, yval, groupsval, model_t, model_reg_zero, model_reg_one, clipping):\n",
    "    \n",
    "    n = Dval.shape[0]\n",
    "    n_treatments = Dtrain.max()\n",
    "    # print(f'n_treatments: {n_treatments}')\n",
    "    \n",
    "    # 0) Fit propensity model and make predictions\n",
    "    reg_prop = model_t(True).fit(Xtrain, Dtrain)\n",
    "    reg_prop_preds = reg_prop.predict(Xval)\n",
    "    # print(reg_prop_preds.shape)\n",
    "    \n",
    "    # 1) Fit control outcome model\n",
    "    reg_zero = model_reg_zero(True).fit(Xtrain[list(Dtrain==0)], ytrain[list(Dtrain==0)])\n",
    "    \n",
    "    # 2) Fit treatment outcome models for each treatment\n",
    "    reg_t = {t: model_reg_one(True).fit(Xtrain[list(Dtrain==t)], ytrain[list(Dtrain==t)]) for t in range(1,n_treatments+1)}\n",
    "    \n",
    "    # 3) Create control and treatment predictions\n",
    "    reg_zero_preds_t = reg_zero.predict(Xval)[:,np.newaxis]\n",
    "    reg_t_preds = {t: reg_t[t].predict(Xval) for t in range(1,n_treatments+1)}\n",
    "    reg_t_preds = np.column_stack(list(reg_t_preds.values()))\n",
    "    \n",
    "    # 4) Calculate differences in predicted outcomes for each treatment relative to control\n",
    "    reg_tau_hat = reg_t_preds - np.tile(reg_zero_preds_t, (1,n_treatments))\n",
    "    \n",
    "    # 5) Calculate residuals\n",
    "    reg_preds_t = np.hstack([reg_zero_preds_t, reg_t_preds])[np.arange(n),Dval]\n",
    "    reg_res = yval - reg_preds_t\n",
    "    \n",
    "    # 6) Calculate IPW adjustment with residuals\n",
    "    reg_prop_preds_t = reg_prop_preds[np.arange(n), Dval]\n",
    "    ipws = np.where(Dval>0,1,-1) * (1/reg_prop_preds_t)\n",
    "    ipw_res_adj = (reg_res*ipws)\n",
    "    \n",
    "    # 7) DR scores\n",
    "    dr_scores = reg_tau_hat\n",
    "    dr_scores[np.arange(n),Dval-1] += ipw_res_adj\n",
    "    \n",
    "    return(dr_scores)\n",
    "    \n",
    "# # TEST\n",
    "# Dm = df_train.custom_treatment.to_numpy()\n",
    "# Xval = df_val[TIDE_VARS].astype(float).to_numpy()\n",
    "# Dval = df_val.custom_treatment.to_numpy()\n",
    "# Yval = df_val['delta_in_range_fw_7d'].values\n",
    "# groupsval = df_val.mrn.values\n",
    "\n",
    "# test_dr = calculate_dr_outcomes_mn(X, Dm, Y, groups, Xval, Dval, Yval, groupsval, model_t, model_y, model_reg_one, clipping=0.01)\n",
    "# print(test_dr)\n",
    "# test_dr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3afee5ff-86fd-4948-802f-d480ee1f2fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scorer\n",
    "def drscore(cate_preds, dr_scores):\n",
    "    \n",
    "    # Only use max CATE preds\n",
    "    max_cate_preds = cate_preds.max(axis=1)\n",
    "    max_cate_pred_drs = dr_scores[np.arange(len(dr_scores)), np.argmax(cate_preds, axis=1)]\n",
    "    \n",
    "    overall_ate_val_dr = np.mean(max_cate_pred_drs)\n",
    "    drscore_t = np.mean((max_cate_pred_drs - max_cate_preds)**2)\n",
    "    drscore_b = np.mean((max_cate_pred_drs - overall_ate_val_dr)**2)\n",
    "    return 1 - drscore_t / drscore_b\n",
    "\n",
    "\n",
    "def get_cate_preds(est, X, n_treatments):\n",
    "    cate_preds = []\n",
    "    for t in range(1, n_treatments+1):\n",
    "        cate_preds.append(est.effect(X,T1=t)[:,np.newaxis])\n",
    "    cate_preds = np.hstack(cate_preds)\n",
    "    return(cate_preds)\n",
    "\n",
    "class Ensemble(BaseEstimator):\n",
    "\n",
    "    def __init__(self, names, models, weights, n_treatments, intercept=0):\n",
    "        self.names = names\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.intercept = intercept\n",
    "        self.n_treatments = n_treatments\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = [get_cate_preds(m, X, self.n_treatments) for m in self.models]\n",
    "        w_preds = [w*p for w, p in zip(self.weights, preds)]\n",
    "        wcate = np.stack(w_preds, axis=0).sum(axis=0)\n",
    "        return self.intercept + wcate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a600f0-8e7b-46ea-a5cb-e811fedf608a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble CATE function\n",
    "def est_ensemble_cate_fn(prop_lb, model_y, model_t, Y, X, W, D, n_splits, groups, Xval, dr_val_scores):\n",
    "    \n",
    "    # Estimate and score input models\n",
    "    scorer = drscore\n",
    "    score_name = 'DRscore'\n",
    "    model_names = [\n",
    "        'CF', 'DRForest', 'XLearner', \n",
    "        'TLearner', 'SLearner']\n",
    "    models = [est_cate_fn(x, prop_lb, model_y, model_t, Y, X, W, D, n_splits, groups) for x in model_names]\n",
    "    scores = [scorer(get_cate_preds(m, Xval, D.max()), dr_val_scores) for m in models]\n",
    "    print([f'{name}: {score:.4f}' for name, score in zip(model_names, scores)])\n",
    "    \n",
    "    # Find the best ensemble based on val DR scores\n",
    "    eta_grid = np.logspace(-5, 5, 10)\n",
    "    ens = {}\n",
    "    for eta in eta_grid:\n",
    "        weights = scipy.special.softmax(eta * np.array(scores)).tolist()\n",
    "        ensemble = Ensemble(model_names, models, weights, D.max())\n",
    "        ens[eta] = (ensemble, scorer(ensemble.predict(Xval), dr_val_scores))\n",
    "\n",
    "    score_best = -np.inf\n",
    "    for eta in eta_grid:\n",
    "        if ens[eta][1] >= score_best:\n",
    "            score_best = ens[eta][1]\n",
    "            eta_best = eta\n",
    "\n",
    "    est = ens[eta_best][0]\n",
    "    \n",
    "    return(est)\n",
    "\n",
    "# # Test\n",
    "# W=X\n",
    "# test_ensemble = est_ensemble_cate_fn(0.01, model_y, model_t, Y, X, W, Dm, n_splits, groups, Xval, test_dr)\n",
    "# test_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3078b606-4cb0-42e4-ae99-b6f307e598ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calc AUTOC and also return TOC curve\n",
    "def toc_df(cate_est, dr_scores, colsuffix):\n",
    "    \n",
    "    # Calc ATE for offset\n",
    "    ate = np.mean(dr_scores)\n",
    "    \n",
    "    # Grid of values of % treated 5-95%\n",
    "    ugrid = np.linspace(5, 95, 50)\n",
    "    \n",
    "    # Quantiles of CATE est corresponding to each treated %\n",
    "    qs = np.percentile(cate_est, ugrid)\n",
    "\n",
    "    # Initialize vectors\n",
    "    toc, toc_std, group_prob = np.zeros(len(qs)), np.zeros(len(qs)), np.zeros(len(qs))\n",
    "    toc_psi = np.zeros((len(qs), dr_scores.shape[0])) # influence function representation of the TOC at each quantile\n",
    "    \n",
    "    # Iterate over %'s\n",
    "    n = len(dr_scores)\n",
    "    for it in range(len(qs)):\n",
    "        inds = (qs[it] <= cate_est) # subset with larger CATE prediction than the q-th quantile\n",
    "        group_prob = np.sum(inds) / n # fraction of population in this group\n",
    "        toc[it]= np.mean(dr_scores[inds]) - ate # Rel ATT at %\n",
    "        # influence function for the tau(q); it is a standard influence function of a \"covariance\"\n",
    "        toc_psi[it, :] = (dr_scores - ate) * (inds / group_prob - 1) - toc[it]\n",
    "        toc_std[it] = np.sqrt(np.mean(toc_psi[it]**2) / n) # standard error of tau(q)\n",
    "        \n",
    "    toc_df = pd.DataFrame({\n",
    "        'p_treated': 100 - ugrid,\n",
    "        'toc': toc,\n",
    "        'toc_std': toc_std,\n",
    "        'toc_lb': toc - 1.96*toc_std,\n",
    "        'toc_ub': toc + 1.96*toc_std,\n",
    "        'att': toc + ate,\n",
    "        'att_lb': toc + ate - 1.96*toc_std,\n",
    "        'att_ub': toc + ate + 1.96*toc_std,\n",
    "        'type': np.repeat([colsuffix], len(ugrid))\n",
    "    })\n",
    "        \n",
    "    # Calculate AUTOC\n",
    "    autoc_psi = np.sum(toc_psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "    autoc = np.sum(toc[:-1] * np.diff(ugrid) / 100)\n",
    "    autoc_stderr = np.sqrt(np.mean(autoc_psi**2) / n)\n",
    "    autoc_df = pd.DataFrame({\n",
    "        f'AUTOC_{colsuffix}': [autoc],\n",
    "        f'AUTOC_se_{colsuffix}' : [autoc_stderr]\n",
    "    })\n",
    "    \n",
    "    return({\n",
    "        'autoc_df': autoc_df,\n",
    "        'toc_df': toc_df\n",
    "    })\n",
    "\n",
    "\n",
    "# Adapt TOC function to handle multiple treatments (3 options + 'best cate pred')\n",
    "\n",
    "def toc_df_mn(mn_cate_est, mn_dr_scores, colsuffix):\n",
    "    \n",
    "    # Get DR scores of max CATE preds\n",
    "    # Combine with DR scores for other treatments\n",
    "    max_test_dr = mn_dr_scores[np.arange(len(mn_dr_scores)), np.argmax(mn_cate_est, axis=1)]\n",
    "    all_drs = np.hstack([mn_dr_scores, max_test_dr[:,np.newaxis]])\n",
    "    # ATEs\n",
    "    # print(all_drs.mean(axis=0))\n",
    "\n",
    "    # Create CATE predictions matrix incl max\n",
    "    max_cate_preds = mn_cate_est.max(axis=1)\n",
    "    all_cate_preds = np.hstack([mn_cate_est, max_cate_preds[:,np.newaxis]])\n",
    "    \n",
    "    # Iterate over treatments and compute toc_df separately for each one\n",
    "    toc_dfs = []\n",
    "    autoc_dfs = []\n",
    "    for t in range(all_cate_preds.shape[1]):\n",
    "        res = toc_df(all_cate_preds[:,t], all_drs[:,t], f'{colsuffix}_t{t+1}')\n",
    "        autoc_dfs.append(res['autoc_df'])\n",
    "        toc_dfs.append(res['toc_df'])\n",
    "        \n",
    "    toc_dfs = pd.concat(toc_dfs, axis=0)\n",
    "    autoc_dfs = pd.concat(autoc_dfs, axis=1)\n",
    "    return({\n",
    "        'autoc_df': autoc_dfs,\n",
    "        'toc_df': toc_dfs\n",
    "    })\n",
    "\n",
    "# Bootstrap\n",
    "\n",
    "def toc_df_mn_bs(mn_cate_est, mn_dr_scores, groups, n_bootstrap_samples, colsuffix):\n",
    "    \n",
    "    unique_groups = np.unique(groups)\n",
    "    bootstrap_toc_dfs = []\n",
    "    for i in range(n_bootstrap_samples):\n",
    "        sampled_groups = np.random.choice(unique_groups, len(unique_groups), replace=True)\n",
    "        sampled_mn_cate_est = np.vstack([\n",
    "            mn_cate_est[groups == g] for g in sampled_groups])\n",
    "        sampled_mn_dr_scores = np.vstack([\n",
    "            mn_dr_scores[groups == g] for g in sampled_groups])\n",
    "        \n",
    "        res_df = toc_df_mn(sampled_mn_cate_est, sampled_mn_dr_scores, colsuffix)\n",
    "        res_df['toc_df']['bs_id'] = i+1\n",
    "        res_df['autoc_df']['bs_id'] = i+1\n",
    "        bootstrap_toc_dfs.append(res_df)\n",
    "        \n",
    "    toc_df = pd.concat([x['toc_df'] for x in bootstrap_toc_dfs])\n",
    "    autoc_df = pd.concat([x['autoc_df'] for x in bootstrap_toc_dfs])\n",
    "    \n",
    "    # Aggregate across bootstrap samples to get STD and LB UB\n",
    "    \n",
    "    toc_df = toc_df.groupby(['p_treated','type'])[['toc','att']].agg(['mean','std']).reset_index()\n",
    "    toc_df.columns = ['p_treated','type','toc','toc_std','att','att_std']\n",
    "    toc_df['toc_lb'] = toc_df.toc - 1.96*toc_df.toc_std\n",
    "    toc_df['toc_ub'] = toc_df.toc + 1.96*toc_df.toc_std\n",
    "    toc_df['att_lb'] = toc_df.att - 1.96*toc_df.att_std\n",
    "    toc_df['att_ub'] = toc_df.att + 1.96*toc_df.att_std\n",
    "    toc_df = toc_df[['p_treated','toc','toc_std','toc_lb','toc_ub','att','att_lb','att_ub','type']]\n",
    "    \n",
    "    n_treatments = mn_cate_est.shape[1]+1\n",
    "    autoc_cols_to_agg = [f'AUTOC_{colsuffix}_t{t+1}' for t in range(n_treatments)]\n",
    "    autoc_means = autoc_df[autoc_cols_to_agg].agg('mean')\n",
    "    autoc_se = autoc_df[autoc_cols_to_agg].agg('std')\n",
    "    autoc_df = pd.DataFrame([autoc_means])\n",
    "    for t in range(n_treatments):\n",
    "        autoc_df[f'AUTOC_se_{colsuffix}_t{t+1}'] = autoc_se[f'AUTOC_{colsuffix}_t{t+1}']\n",
    "    \n",
    "    return({\n",
    "        'toc_df': toc_df,\n",
    "        'autoc_df': autoc_df\n",
    "    })\n",
    "\n",
    "# # TEST\n",
    "# (p9.ggplot(toc_df_mn(est_cates, test_dr, 'val')['toc_df'],\n",
    "#            p9.aes(x='p_treated', y='att', ymin='att_lb', ymax='att_ub', color='type')) + \n",
    "#      p9.geom_line() + p9.geom_pointrange() + p9.theme_bw() + p9.geom_hline(yintercept=0, linetype='dashed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6937f7a-7650-46a4-ba11-5f00d20d2e82",
   "metadata": {},
   "source": [
    "# Variable Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7a7ce94-84af-4591-a394-a04a0b54846b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STATE_REPS = {\n",
    "    'TIDE (+3w lags in control covars)': TIDE_VARS,\n",
    "    'TIDE w Lags': TIDE_VARS + ['lag_1_very_low_7dr','lag_1_low_7dr','lag_1_in_range_7dr','lag_1_g_7dr','lag_1_in_range_7dr_7d_delta','lag_1_large_tir_drop','lag_1_low_tir','lag_1_lows','lag_1_very_lows','lag_1_received_message','lag_2_very_low_7dr','lag_2_low_7dr','lag_2_in_range_7dr','lag_2_g_7dr','lag_2_in_range_7dr_7d_delta','lag_2_large_tir_drop','lag_2_low_tir','lag_2_lows','lag_2_very_lows','lag_2_received_message','lag_3_very_low_7dr','lag_3_low_7dr','lag_3_in_range_7dr','lag_3_g_7dr','lag_3_in_range_7dr_7d_delta','lag_3_large_tir_drop','lag_3_low_tir','lag_3_lows','lag_3_very_lows','lag_3_received_message','lag_4_very_low_7dr','lag_4_low_7dr','lag_4_in_range_7dr','lag_4_g_7dr','lag_4_in_range_7dr_7d_delta','lag_4_large_tir_drop','lag_4_low_tir','lag_4_lows', 'lag_4_very_lows', 'lag_4_received_message'],\n",
    "    'TIDE 1w Lags': TIDE_VARS + \n",
    "        ['lag_1_very_low_7dr','lag_1_low_7dr','lag_1_in_range_7dr','lag_1_g_7dr','lag_1_in_range_7dr_7d_delta','lag_1_large_tir_drop','lag_1_low_tir','lag_1_lows','lag_1_very_lows','lag_1_received_message'],\n",
    "    'TIDE 2w Lags': TIDE_VARS + \\\n",
    "        ['lag_1_very_low_7dr','lag_1_low_7dr','lag_1_in_range_7dr','lag_1_g_7dr','lag_1_in_range_7dr_7d_delta','lag_1_large_tir_drop','lag_1_low_tir','lag_1_lows','lag_1_very_lows','lag_1_received_message'] + \\\n",
    "        ['lag_2_very_low_7dr','lag_2_low_7dr','lag_2_in_range_7dr','lag_2_g_7dr','lag_2_in_range_7dr_7d_delta','lag_2_large_tir_drop','lag_2_low_tir','lag_2_lows','lag_2_very_lows','lag_2_received_message'],\n",
    "    'Expert (full)': ['g_7dr', 'very_low_7dr', 'low_7dr', 'in_range_7dr', 'high_7dr', 'very_high_7dr', 'gri_7dr', 'g_14dr', 'very_low_14dr', 'low_14dr', 'in_range_14dr', 'high_14dr', 'very_high_14dr', 'gri_14dr', 'night_very_low_7dr', 'night_low_7dr', 'night_high_7dr', 'night_very_high_7dr', 'day_very_low_7dr', 'day_low_7dr', 'day_high_7dr', 'day_very_high_7dr', 'time_worn_7dr', 'night_worn_7dr', 'day_worn_7dr', 'sexF', 'public_insurance', 'english_primary_language', 'pop_pilot', 'pop_4T_1', 'pop_4T_2', 'pop_TIPS', 'age', 'months_since_onset', 'using_pump', 'using_aid', 'large_tir_drop', 'low_tir', 'lows', 'very_lows'],\n",
    "    'Expert (subset)': ['large_tir_drop', 'in_range_14dr', 'in_range_7dr', 'low_7dr', 'using_pump', 'time_worn_7dr', 'day_worn_7dr', 'day_low_7dr', 'g_7dr', 'months_since_onset'],\n",
    "    'Raw (UMAP) (+3w lags in control covars)': ['umap0', 'umap1', 'umap2', 'umap3'],\n",
    "    'Raw (TS2Vec) (+3w lags in control covars)': ['ts2vec_0', 'ts2vec_1', 'ts2vec_2', 'ts2vec_3', 'ts2vec_4', 'ts2vec_5',\n",
    "       'ts2vec_6', 'ts2vec_7']\n",
    "}\n",
    "\n",
    "ACTION_REPS = {\n",
    "    'Expert v3': 'custom_treatment_v3',\n",
    "    'Binary (any)': 'received_message',\n",
    "    'Clustered Embeddings (2)': 'km_treatment_2',\n",
    "    'Clustered Embeddings (3)': 'km_treatment_3',\n",
    "    'Clustered Embeddings (4)': 'km_treatment_4',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35027a4e-b071-466c-8666-699bb72d9eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def driver(df, outcome, state_rep, action_rep, \n",
    "           cate_estimator, p_wins, prop_lb):\n",
    "    \n",
    "    ####################\n",
    "    ### PREPARE DATA ###\n",
    "    ####################\n",
    "    \n",
    "    # Define reward (outcome) and winsorize\n",
    "    for d in [df_train, df_val, df_test]:\n",
    "        d['reward'] = d[outcome]\n",
    "    lb, ub = df_train.reward.quantile([p_wins, 1-p_wins])\n",
    "    for d in [df_train, df_val, df_test]:\n",
    "        d['reward'] = d.reward.clip(lb, ub)\n",
    "\n",
    "    X = df_train[STATE_REPS[state_rep]].astype(float).fillna(0.0).to_numpy()\n",
    "    W = df_train[CONTROL_VARS].astype(float).to_numpy()\n",
    "    Y = df_train['reward'].values\n",
    "    D = df_train[ACTION_REPS[action_rep]].astype(int).values\n",
    "    groups = df_train.mrn.values\n",
    "\n",
    "    Xval = df_val[STATE_REPS[state_rep]].astype(float).fillna(0.0).to_numpy()\n",
    "    Wval = df_val[CONTROL_VARS].astype(float).to_numpy()\n",
    "    Yval = df_val['reward'].values\n",
    "    Dval = df_val[ACTION_REPS[action_rep]].astype(int).values\n",
    "    groupsval = df_val.mrn.values\n",
    "\n",
    "    Xtest = df_test[STATE_REPS[state_rep]].astype(float).fillna(0.0).to_numpy()\n",
    "    Wtest = df_test[CONTROL_VARS].astype(float).to_numpy()\n",
    "    Ytest = df_test['reward'].values\n",
    "    Dtest = df_test[ACTION_REPS[action_rep]].astype(int).values\n",
    "    groupstest = df_test.mrn.values\n",
    "    \n",
    "    # print(groups.shape)\n",
    "    # print(np.unique(groups).shape)\n",
    "    # print(groupsval.shape)\n",
    "    # print(np.unique(groupsval).shape)\n",
    "    # print(groupstest.shape)\n",
    "    # print(np.unique(groupstest).shape)\n",
    "    \n",
    "    #######################################\n",
    "    ### Fit and Evaluate CATE Estimator ###\n",
    "    #######################################\n",
    "    \n",
    "    # Calculate DR scores\n",
    "    dr_val = calculate_dr_outcomes_mn(W, D, Y, groups, Wval, Dval, Yval, groupsval, model_t, model_y, model_reg_one, clipping=prop_lb)\n",
    "    dr_test = calculate_dr_outcomes_mn(W, D, Y, groups, Wtest, Dtest, Ytest, groupstest, model_t, model_y, model_reg_one, clipping=prop_lb)\n",
    "\n",
    "    # Train estimator\n",
    "    if cate_estimator != 'Ensemble':\n",
    "        est = est_cate_fn(cate_estimator, prop_lb, model_y, model_t, Y, X, W, D, n_splits, groups)\n",
    "    else:\n",
    "        est = est_ensemble_cate_fn(prop_lb, model_y, model_t, Y, X, W, D, n_splits, groups, Xval, dr_val)\n",
    "    \n",
    "    # Make CATE predictions\n",
    "    if cate_estimator != 'Ensemble':\n",
    "        cate_preds_val = []\n",
    "        for t in range(1, D.max()+1):\n",
    "            cate_preds_val.append(est.effect(Xval,T1=t)[:,np.newaxis])\n",
    "        cate_preds_val = np.hstack(cate_preds_val)\n",
    "        \n",
    "        cate_preds_test = []\n",
    "        for t in range(1, D.max()+1):\n",
    "            cate_preds_test.append(est.effect(Xtest,T1=t)[:,np.newaxis])\n",
    "        cate_preds_test = np.hstack(cate_preds_test)\n",
    "        \n",
    "    else:\n",
    "        cate_preds_val = est.predict(Xval)\n",
    "        cate_preds_test = est.predict(Xtest)\n",
    "\n",
    "    # AUTOC and TOC\n",
    "    toc_val = toc_df_mn_bs(cate_preds_val, dr_val, groupsval, 500, 'val')\n",
    "    toc_test = toc_df_mn_bs(cate_preds_test, dr_test, groupstest, 500, 'test')\n",
    "    \n",
    "    ################################\n",
    "    ### Create output dataframes ###\n",
    "    ################################\n",
    "    \n",
    "    params_df = pd.DataFrame(\n",
    "        [[outcome, state_rep, action_rep, cate_estimator, p_wins, prop_lb]],\n",
    "        columns=['outcome', 'state_rep', 'action_rep', 'cate_estimator', 'p_wins', 'prop_lb'])\n",
    "    \n",
    "    # Combine dataframes\n",
    "    summary_df = pd.concat(\n",
    "        [params_df, toc_val['autoc_df'], toc_test['autoc_df'], \n",
    "         # lproj_df\n",
    "        ],\n",
    "        axis=1)\n",
    "    \n",
    "    toc_df_val = toc_val['toc_df']\n",
    "    toc_df_test = toc_test['toc_df']\n",
    "    toc_df_val['dataset'] = 'val'\n",
    "    toc_df_test['dataset'] = 'test'\n",
    "    toc_df = pd.concat([toc_df_val, toc_df_test], ignore_index=True)\n",
    "    toc_df = toc_df.assign(\n",
    "        outcome = np.repeat(outcome, len(toc_df)),\n",
    "        state_rep = np.repeat(state_rep, len(toc_df)),\n",
    "        action_rep = np.repeat(action_rep, len(toc_df)),\n",
    "        cate_estimator = np.repeat(cate_estimator, len(toc_df)),\n",
    "        p_wins = np.repeat(p_wins, len(toc_df)),\n",
    "        prop_lb = np.repeat(prop_lb, len(toc_df)),\n",
    "    )\n",
    "\n",
    "    return({\n",
    "        'summary_df': summary_df,\n",
    "        'toc_df': toc_df\n",
    "    })\n",
    "\n",
    "\n",
    "# # TEST\n",
    "# TESTING = False\n",
    "# t = driver(\n",
    "#     df,\n",
    "#     outcome = 'delta_in_range_fw_7d',\n",
    "#     state_rep = 'TIDE',\n",
    "#     action_rep = 'Expert v2',\n",
    "#     cate_estimator = 'Ensemble',\n",
    "#     p_wins = 0.1,\n",
    "#     prop_lb = 0.02\n",
    "# )\n",
    "# pd.set_option('display.max_columns', 50)\n",
    "# print(t['summary_df'])\n",
    "# t['toc_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a79ea-4fd0-436f-952b-7ff9d4e332e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Grid\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "outcomes = [\n",
    "    'delta_in_range_fw_7d', \n",
    "]\n",
    "state_reps = list(STATE_REPS.keys())\n",
    "action_reps = list(ACTION_REPS.keys())\n",
    "cate_estimators = [\n",
    "    'Ensemble',\n",
    "    'TLearner', \n",
    "    'CF', \n",
    "    'DRForest', \n",
    "    'XLearner', \n",
    "    'SLearner',\n",
    "]\n",
    "p_wins = [0.05]\n",
    "prop_lb = [0.01]\n",
    "\n",
    "grid = list(product(\n",
    "    outcomes, state_reps, action_reps, cate_estimators, p_wins, prop_lb\n",
    "))\n",
    "\n",
    "print(len(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2251335-8f9a-4a7c-bd0d-cbb4e281ccef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Expert', 'CF', 0.1, 0.01)\n",
      "1 of 180 runs succeeded so far\n",
      "1\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Expert', 'DRForest', 0.1, 0.01)\n",
      "2 of 180 runs succeeded so far\n",
      "2\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Expert', 'XLearner', 0.1, 0.01)\n",
      "3 of 180 runs succeeded so far\n",
      "3\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Expert', 'TLearner', 0.1, 0.01)\n",
      "4 of 180 runs succeeded so far\n",
      "4\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Expert', 'SLearner', 0.1, 0.01)\n",
      "5 of 180 runs succeeded so far\n",
      "5\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (any)', 'CF', 0.1, 0.01)\n",
      "6 of 180 runs succeeded so far\n",
      "6\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (any)', 'DRForest', 0.1, 0.01)\n",
      "7 of 180 runs succeeded so far\n",
      "7\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (any)', 'XLearner', 0.1, 0.01)\n",
      "8 of 180 runs succeeded so far\n",
      "8\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (any)', 'TLearner', 0.1, 0.01)\n",
      "9 of 180 runs succeeded so far\n",
      "9\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (any)', 'SLearner', 0.1, 0.01)\n",
      "10 of 180 runs succeeded so far\n",
      "10\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (dose change)', 'CF', 0.1, 0.01)\n",
      "11 of 180 runs succeeded so far\n",
      "11\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (dose change)', 'DRForest', 0.1, 0.01)\n",
      "12 of 180 runs succeeded so far\n",
      "12\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (dose change)', 'XLearner', 0.1, 0.01)\n",
      "13 of 180 runs succeeded so far\n",
      "13\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (dose change)', 'TLearner', 0.1, 0.01)\n",
      "14 of 180 runs succeeded so far\n",
      "14\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Binary (dose change)', 'SLearner', 0.1, 0.01)\n",
      "15 of 180 runs succeeded so far\n",
      "15\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (2)', 'CF', 0.1, 0.01)\n",
      "16 of 180 runs succeeded so far\n",
      "16\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (2)', 'DRForest', 0.1, 0.01)\n",
      "17 of 180 runs succeeded so far\n",
      "17\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (2)', 'XLearner', 0.1, 0.01)\n",
      "18 of 180 runs succeeded so far\n",
      "18\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (2)', 'TLearner', 0.1, 0.01)\n",
      "19 of 180 runs succeeded so far\n",
      "19\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (2)', 'SLearner', 0.1, 0.01)\n",
      "20 of 180 runs succeeded so far\n",
      "20\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (3)', 'CF', 0.1, 0.01)\n",
      "21 of 180 runs succeeded so far\n",
      "21\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (3)', 'DRForest', 0.1, 0.01)\n",
      "22 of 180 runs succeeded so far\n",
      "22\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (3)', 'XLearner', 0.1, 0.01)\n",
      "23 of 180 runs succeeded so far\n",
      "23\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (3)', 'TLearner', 0.1, 0.01)\n",
      "24 of 180 runs succeeded so far\n",
      "24\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (3)', 'SLearner', 0.1, 0.01)\n",
      "25 of 180 runs succeeded so far\n",
      "25\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (4)', 'CF', 0.1, 0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS warning: precompiled NUM_THREADS exceeded, adding auxiliary array for thread metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 of 180 runs succeeded so far\n",
      "26\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (4)', 'DRForest', 0.1, 0.01)\n",
      "27 of 180 runs succeeded so far\n",
      "27\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (4)', 'XLearner', 0.1, 0.01)\n",
      "28 of 180 runs succeeded so far\n",
      "28\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (4)', 'TLearner', 0.1, 0.01)\n",
      "29 of 180 runs succeeded so far\n",
      "29\n",
      "('delta_in_range_fw_7d', 'TIDE', 'Clustered Embeddings (4)', 'SLearner', 0.1, 0.01)\n",
      "30 of 180 runs succeeded so far\n",
      "30\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Expert', 'CF', 0.1, 0.01)\n",
      "31 of 180 runs succeeded so far\n",
      "31\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Expert', 'DRForest', 0.1, 0.01)\n",
      "32 of 180 runs succeeded so far\n",
      "32\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Expert', 'XLearner', 0.1, 0.01)\n",
      "33 of 180 runs succeeded so far\n",
      "33\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Expert', 'TLearner', 0.1, 0.01)\n",
      "34 of 180 runs succeeded so far\n",
      "34\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Expert', 'SLearner', 0.1, 0.01)\n",
      "35 of 180 runs succeeded so far\n",
      "35\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (any)', 'CF', 0.1, 0.01)\n",
      "36 of 180 runs succeeded so far\n",
      "36\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (any)', 'DRForest', 0.1, 0.01)\n",
      "37 of 180 runs succeeded so far\n",
      "37\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (any)', 'XLearner', 0.1, 0.01)\n",
      "38 of 180 runs succeeded so far\n",
      "38\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (any)', 'TLearner', 0.1, 0.01)\n",
      "39 of 180 runs succeeded so far\n",
      "39\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (any)', 'SLearner', 0.1, 0.01)\n",
      "40 of 180 runs succeeded so far\n",
      "40\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (dose change)', 'CF', 0.1, 0.01)\n",
      "41 of 180 runs succeeded so far\n",
      "41\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (dose change)', 'DRForest', 0.1, 0.01)\n",
      "42 of 180 runs succeeded so far\n",
      "42\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (dose change)', 'XLearner', 0.1, 0.01)\n",
      "43 of 180 runs succeeded so far\n",
      "43\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (dose change)', 'TLearner', 0.1, 0.01)\n",
      "44 of 180 runs succeeded so far\n",
      "44\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Binary (dose change)', 'SLearner', 0.1, 0.01)\n",
      "45 of 180 runs succeeded so far\n",
      "45\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (2)', 'CF', 0.1, 0.01)\n",
      "46 of 180 runs succeeded so far\n",
      "46\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (2)', 'DRForest', 0.1, 0.01)\n",
      "47 of 180 runs succeeded so far\n",
      "47\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (2)', 'XLearner', 0.1, 0.01)\n",
      "48 of 180 runs succeeded so far\n",
      "48\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (2)', 'TLearner', 0.1, 0.01)\n",
      "49 of 180 runs succeeded so far\n",
      "49\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (2)', 'SLearner', 0.1, 0.01)\n",
      "50 of 180 runs succeeded so far\n",
      "50\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (3)', 'CF', 0.1, 0.01)\n",
      "51 of 180 runs succeeded so far\n",
      "51\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (3)', 'DRForest', 0.1, 0.01)\n",
      "52 of 180 runs succeeded so far\n",
      "52\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (3)', 'XLearner', 0.1, 0.01)\n",
      "53 of 180 runs succeeded so far\n",
      "53\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (3)', 'TLearner', 0.1, 0.01)\n",
      "54 of 180 runs succeeded so far\n",
      "54\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (3)', 'SLearner', 0.1, 0.01)\n",
      "55 of 180 runs succeeded so far\n",
      "55\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (4)', 'CF', 0.1, 0.01)\n",
      "56 of 180 runs succeeded so far\n",
      "56\n",
      "('delta_in_range_fw_7d', 'Expert (full)', 'Clustered Embeddings (4)', 'DRForest', 0.1, 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Run whole grid\n",
    "TESTING = False\n",
    "N_SUCCESSES = 0\n",
    "PREFIX=''\n",
    "FOLDER='eval_results'\n",
    "\n",
    "output_dir = FOLDER\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)  \n",
    "\n",
    "# for i in range(len(grid)):\n",
    "for i in range(len(grid)):\n",
    "    print(i)\n",
    "    print(grid[i])\n",
    "    res = driver(df, *grid[i])\n",
    "    try:\n",
    "        res = driver(df, *grid[i])\n",
    "        res['summary_df'].to_csv(f'{output_dir}/{PREFIX}_{i}_summary.csv', index=False)\n",
    "        res['toc_df'].to_csv(f'{output_dir}/{PREFIX}_{i}_toc.csv', index=False)\n",
    "        N_SUCCESSES += 1\n",
    "        print(f'{N_SUCCESSES} of {len(grid)} runs succeeded so far')\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "\n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
